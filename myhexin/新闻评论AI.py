#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–°é—»è¯„è®ºç”Ÿæˆå™¨
åŠŸèƒ½ï¼šåˆ†ææ–°é—»å†…å®¹ï¼Œç”Ÿæˆæœ‰è¶£çš„è¯„è®ºï¼Œå¹¶é…å¥—ç›¸å…³å›¾ç‰‡
"""

import requests
import json
import re
import time
import random
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import openai
from PIL import Image, ImageDraw, ImageFont
import io
import base64
from typing import Dict, List, Optional, Tuple
import praw
from dataclasses import dataclass
from datetime import datetime


@dataclass
class NewsAnalysis:
    """æ–°é—»åˆ†æç»“æœæ•°æ®ç±»"""
    humor_points: List[str]  # ç¬‘ç‚¹
    criticism_points: List[str]  # æ§½ç‚¹
    core_viewpoints: List[str]  # æ ¸å¿ƒè§‚ç‚¹
    controversial_points: List[str]  # äº‰è®®ç‚¹
    key_images: List[str]  # å…³é”®å›¾ç‰‡URL
    summary: str  # æ–‡ç« æ‘˜è¦


@dataclass
class RedditReference:
    """Redditå‚è€ƒè¯„è®ºæ•°æ®ç±»"""
    comment: str
    score: int
    awards: int
    subreddit: str
    style: str  # è¯„è®ºé£æ ¼æ ‡ç­¾


@dataclass
class GeneratedComment:
    """ç”Ÿæˆçš„è¯„è®ºæ•°æ®ç±»"""
    content: str
    style: str  # é£æ ¼ï¼šprovocative, witty, insightful, question
    image_prompt: str  # é…å¥—å›¾ç‰‡æè¿°
    confidence: float  # è´¨é‡è¯„åˆ†


class NewsContentExtractor:
    """æ–°é—»å†…å®¹æå–å™¨"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def extract_content(self, url: str) -> Dict:
        """æå–æ–°é—»å†…å®¹å’Œå›¾ç‰‡"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            # æå–æ ‡é¢˜
            title = self._extract_title(soup)

            # æå–æ­£æ–‡
            content = self._extract_article_text(soup)

            # æå–å›¾ç‰‡
            images = self._extract_images(soup, url)

            return {
                'title': title,
                'content': content,
                'images': images,
                'url': url
            }

        except Exception as e:
            raise Exception(f"å†…å®¹æå–å¤±è´¥: {str(e)}")

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–æ ‡é¢˜"""
        selectors = ['h1', '.title', '#title', '[class*="title"]', 'title']
        for selector in selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()
        return "æœªæ‰¾åˆ°æ ‡é¢˜"

    def _extract_article_text(self, soup: BeautifulSoup) -> str:
        """æå–æ–‡ç« æ­£æ–‡"""
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()

        # å°è¯•å¤šç§æ­£æ–‡é€‰æ‹©å™¨
        content_selectors = [
            '[class*="content"]', '[class*="article"]', '[class*="story"]',
            '[id*="content"]', '[id*="article"]', 'main', '.post-content'
        ]

        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                text = ' '.join([elem.get_text() for elem in elements])
                if len(text) > 200:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                    return self._clean_text(text)

        # å¤‡ç”¨æ–¹æ¡ˆï¼šæå–æ‰€æœ‰pæ ‡ç­¾
        paragraphs = soup.find_all('p')
        if paragraphs:
            text = ' '.join([p.get_text() for p in paragraphs])
            return self._clean_text(text)

        return soup.get_text()[:2000]  # é™åˆ¶é•¿åº¦

    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """æå–å›¾ç‰‡URL"""
        images = []
        img_tags = soup.find_all('img')

        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src:
                full_url = urljoin(base_url, src)
                if self._is_valid_image(full_url):
                    images.append(full_url)

        return images[:5]  # é™åˆ¶å›¾ç‰‡æ•°é‡

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶ç©ºç™½å­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()"""''â€”-]', '', text)  # ä¿ç•™ä¸­è‹±æ–‡å’Œå¸¸ç”¨æ ‡ç‚¹
        return text.strip()

    def _is_valid_image(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå›¾ç‰‡URL"""
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
        return any(url.lower().endswith(ext) for ext in valid_extensions)


class NewsAnalyzer:
    """æ–°é—»å†…å®¹åˆ†æå™¨"""

    def __init__(self, api_key: str):
        openai.api_key = api_key

    def analyze_news(self, title: str, content: str) -> NewsAnalysis:
        """åˆ†ææ–°é—»å†…å®¹ï¼Œè¯†åˆ«å„ç§è¦ç‚¹"""

        prompt = f"""
        è¯·ç”¨ä¸­æ–‡åˆ†æä»¥ä¸‹æ–°é—»å†…å®¹ï¼Œè¯†åˆ«å‡ºï¼š
        1. ç¬‘ç‚¹ï¼ˆæœ‰è¶£ã€æç¬‘çš„éƒ¨åˆ†ï¼‰
        2. æ§½ç‚¹ï¼ˆå€¼å¾—åæ§½ã€æ‰¹è¯„çš„åœ°æ–¹ï¼‰
        3. æ ¸å¿ƒè§‚ç‚¹ï¼ˆæ–‡ç« çš„ä¸»è¦è®ºç‚¹ï¼‰
        4. æ½œåœ¨äº‰è®®ç‚¹ï¼ˆå¯èƒ½å¼•èµ·äº‰è®®çš„å†…å®¹ï¼‰

        æ ‡é¢˜ï¼š{title}
        å†…å®¹ï¼š{content[:2000]}

        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - humor_points: ç¬‘ç‚¹åˆ—è¡¨
        - criticism_points: æ§½ç‚¹åˆ—è¡¨
        - core_viewpoints: æ ¸å¿ƒè§‚ç‚¹åˆ—è¡¨
        - controversial_points: äº‰è®®ç‚¹åˆ—è¡¨
        - summary: æ–‡ç« æ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰
        """

        try:
            response = openai.chat.completions.create(
                model="gpt-5",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆï¼Œå–„äºè¯†åˆ«æ–°é—»ä¸­çš„å„ç§è¦ç‚¹ã€‚è¯·ç”¨ä¸­æ–‡åˆ†æï¼Œå¹¶è¯·å§‹ç»ˆè¿”å›æ ‡å‡†JSONæ ¼å¼å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ³¨é‡Šæˆ–markdownæ ¼å¼ï¼Œä¸åŠ è§£é‡Šå’Œä»»ä½•è¯´æ˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=1
            )
            raw_content = response.choices[0].message.content.strip()

            # å¦‚æœä»¥```å¼€å¤´ï¼Œå»é™¤Markdownä»£ç å—
            if raw_content.startswith("```"):
                raw_content = re.sub(r"^```(?:json)?\s*", "", raw_content)  # ç§»é™¤èµ·å§‹ ```
                raw_content = re.sub(r"\s*```$", "", raw_content)  # ç§»é™¤ç»“æŸ ```

            result = json.loads(raw_content)

            return NewsAnalysis(
                humor_points=result.get('humor_points', []),
                criticism_points=result.get('criticism_points', []),
                core_viewpoints=result.get('core_viewpoints', []),
                controversial_points=result.get('controversial_points', []),
                key_images=[],  # å°†åœ¨åç»­å¡«å……
                summary=result.get('summary', '')
            )

        except Exception as e:
            print(f"æ–°é—»åˆ†æå¤±è´¥: {e}")
            return NewsAnalysis([], [], [], [], [], "åˆ†æå¤±è´¥")


class RedditMiner:
    """Redditè¯„è®ºæŒ–æ˜å™¨"""

    def __init__(self, client_id: str, client_secret: str, user_agent: str):
        self.reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )

    def find_related_discussions(self, keywords: List[str], limit: int = 10) -> List[RedditReference]:
        """æ ¹æ®å…³é”®è¯æœç´¢ç›¸å…³Redditè®¨è®º"""
        references = []

        # ç»„åˆå…³é”®è¯è¿›è¡Œæœç´¢
        query = ' OR '.join(keywords[:3])  # é™åˆ¶å…³é”®è¯æ•°é‡

        try:
            # æœç´¢çƒ­é—¨å¸–å­
            for submission in self.reddit.subreddit('all').search(query, limit=limit, sort='hot'):
                # è·å–é«˜è´¨é‡è¯„è®º
                submission.comments.replace_more(limit=0)  # å±•å¼€è¯„è®º

                for comment in submission.comments[:5]:  # å–å‰5ä¸ªè¯„è®º
                    if len(comment.body) > 50 and comment.score > 10:
                        style = self._classify_comment_style(comment.body)

                        references.append(RedditReference(
                            comment=comment.body[:300],  # é™åˆ¶é•¿åº¦
                            score=comment.score,
                            awards=len(comment.all_awardings) if hasattr(comment, 'all_awardings') else 0,
                            subreddit=submission.subreddit.display_name,
                            style=style
                        ))

            # æŒ‰çƒ­åº¦æ’åº
            references.sort(key=lambda x: x.score + x.awards * 10, reverse=True)
            return references[:5]  # è¿”å›æœ€çƒ­é—¨çš„5ä¸ª

        except Exception as e:
            print(f"Redditæœç´¢å¤±è´¥: {e}")
            return []

    def _classify_comment_style(self, comment: str) -> str:
        """åˆ†ç±»è¯„è®ºé£æ ¼"""
        comment_lower = comment.lower()

        # å¼•æˆ˜å‹
        if any(word in comment_lower for word in ['stupid', 'wrong', 'disagree', 'ridiculous']):
            return 'provocative'

        # å¹½é»˜å‹
        elif any(word in comment_lower for word in ['lol', 'funny', 'joke', 'haha', 'ğŸ˜‚']):
            return 'witty'

        # æé—®å‹
        elif comment.count('?') >= 2:
            return 'question'

        # æ·±åˆ»å‹
        elif len(comment) > 200:
            return 'insightful'

        return 'neutral'


class CommentGenerator:
    """è¯„è®ºç”Ÿæˆå™¨"""

    def __init__(self, api_key: str):
        openai.api_key = api_key
        self.styles = ['provocative', 'witty', 'insightful', 'question']

    def generate_comments(self, analysis: NewsAnalysis, reddit_refs: List[RedditReference]) -> List[GeneratedComment]:
        """ç”Ÿæˆå¤šç§é£æ ¼çš„è¯„è®º"""
        comments = []

        # ä¸ºæ¯ç§é£æ ¼ç”Ÿæˆä¸€ä¸ªè¯„è®º
        for style in self.styles:
            comment = self._generate_single_comment(analysis, reddit_refs, style)
            if comment:
                comments.append(comment)

        return sorted(comments, key=lambda x: x.confidence, reverse=True)

    def _generate_single_comment(self, analysis: NewsAnalysis, reddit_refs: List[RedditReference], style: str) -> \
    Optional[GeneratedComment]:
        """ç”Ÿæˆç‰¹å®šé£æ ¼çš„è¯„è®º"""

        # è·å–ç›¸åŒé£æ ¼çš„Redditå‚è€ƒ
        style_refs = [ref for ref in reddit_refs if ref.style == style]
        ref_text = '\n'.join([ref.comment for ref in style_refs[:2]])

        # æ„å»ºæç¤º
        style_prompts = {
            'provocative': 'ç”Ÿæˆä¸€ä¸ªæœ‰äº‰è®®æ€§ã€èƒ½å¤Ÿå¼•èµ·è®¨è®ºçš„è¯„è®ºï¼Œè¦çŠ€åˆ©ä½†ä¸è¿‡åˆ†',
            'witty': 'ç”Ÿæˆä¸€ä¸ªæœºæ™ºå¹½é»˜çš„è¯„è®ºï¼Œå¯ä»¥æ˜¯åŒå…³è¯­ã€è®½åˆºæˆ–å·§å¦™çš„è§‚å¯Ÿ',
            'insightful': 'ç”Ÿæˆä¸€ä¸ªæ·±åˆ»æœ‰è§åœ°çš„è¯„è®ºï¼Œæä¾›æ–°çš„è§†è§’æˆ–æ·±åº¦åˆ†æ',
            'question': 'ç”Ÿæˆä¸€ä¸ªå‘äººæ·±çœçš„é—®é¢˜ï¼Œèƒ½å¤Ÿå¼•å‘è¯»è€…æ€è€ƒ'
        }

        prompt = f"""
        # è§’è‰²
        ä½ æ˜¯ä¸€åæ´å¯ŸåŠ›æ•é”ã€è¯­è¨€é£è¶£çš„ç¤¾äº¤åª’ä½“è¯„è®ºå‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºä¸€ä»½æ–°é—»çš„æ ¸å¿ƒåˆ†æï¼Œåˆ›ä½œå‡ºä¸€æ¡ç®€çŸ­ã€çŠ€åˆ©ã€æ˜“äºä¼ æ’­çš„åŸåˆ›è¯„è®ºï¼Œå¹¶ä¸ºä¹‹æ„æ€ä¸€ä¸ªå¯Œæœ‰åˆ›æ„çš„å›¾ç‰‡æè¿°ã€‚

        # æ–°é—»åˆ†æææ–™
        - **æ ¸å¿ƒè§‚ç‚¹**: {'; '.join(analysis.core_viewpoints)}
        - **äº‰è®®ä¸æ§½ç‚¹**: {'; '.join(analysis.controversial_points + analysis.criticism_points)}
        - **å¹½é»˜ä¸ç¬‘ç‚¹**: {'; '.join(analysis.humor_points)}

        # åˆ›ä½œæŒ‡ä»¤
        1.  **è¯„è®ºå†…å®¹**:
            - **é£æ ¼**: æ•´ä½“åŸºè°ƒä¸º **{style_prompts[style]}**ã€‚
            - **å‚è€ƒ**: æ¨¡ä»¿ä»¥ä¸‹ä¾‹å­çš„è¯­æ°”å’Œæ€åº¦ï¼Œä½†**ä¸è¦ç…§æŠ„**å†…å®¹æˆ–å¥å¼: "{ref_text}"
            - **æ•´åˆ**: å·§å¦™åœ°èåˆ`æ–°é—»åˆ†æææ–™`ä¸­çš„è‡³å°‘ä¸€åˆ°ä¸¤ä¸ªå…³é”®ç‚¹ï¼Œå½¢æˆè¿è´¯ã€æœ‰è§åœ°çš„è§‚ç‚¹ã€‚é¿å…ç”Ÿç¡¬ç½—åˆ—ã€‚
            - **çº¦æŸ**: é•¿åº¦ä¸¥æ ¼æ§åˆ¶åœ¨100å­—ä»¥å†…ï¼Œè¯­è¨€è‡ªç„¶å£è¯­åŒ–ï¼Œç¬¦åˆç½‘ç»œè®¨è®ºä¹ æƒ¯ã€‚

        2.  **å›¾ç‰‡æè¿° (Image Prompt)**:
            - **è¦æ±‚**: æ„æ€ä¸€ä¸ªå…·æœ‰è±¡å¾æ€§ã€è®½åˆºæ€§æˆ–æˆå‰§æ€§å¼ åŠ›çš„ç”»é¢ï¼Œèƒ½å¤Ÿå°†ä½ çš„è¯„è®ºæ ¸å¿ƒæ€æƒ³è§†è§‰åŒ–ã€‚æè¿°éœ€è¦ç”ŸåŠ¨ã€å¯Œæœ‰æƒ³è±¡åŠ›ï¼Œä¾¿äºAIç»˜ç”»æ¨¡å‹ç†è§£ã€‚

        # è¾“å‡ºæ ¼å¼
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•JSONæ ¼å¼ä¹‹å¤–çš„é¢å¤–è§£é‡Šã€‚
        {{
            "comment": "åœ¨è¿™é‡Œå¡«å†™ä½ åˆ›ä½œçš„è¯„è®ºå†…å®¹",
            "image_prompt": "åœ¨è¿™é‡Œå¡«å†™ä½ æ„æ€çš„å›¾ç‰‡æè¿°",
            "confidence": 0.8
        }}
        """

        try:
            response = openai.chat.completions.create(
                model="gpt-5",
                messages=[
                    {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ª{style}é£æ ¼çš„ç½‘ç»œè¯„è®ºé«˜æ‰‹ã€‚è¯·å§‹ç»ˆè¿”å›æ ‡å‡†JSONæ ¼å¼å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ³¨é‡Šæˆ–markdownæ ¼å¼ï¼Œä¸åŠ è§£é‡Šå’Œä»»ä½•è¯´æ˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=1
            )

            raw_content = response.choices[0].message.content.strip()

            # å¦‚æœä»¥```å¼€å¤´ï¼Œå»é™¤Markdownä»£ç å—
            if raw_content.startswith("```"):
                raw_content = re.sub(r"^```(?:json)?\s*", "", raw_content)  # ç§»é™¤èµ·å§‹ ```
                raw_content = re.sub(r"\s*```$", "", raw_content)  # ç§»é™¤ç»“æŸ ```

            result = json.loads(raw_content)

            return GeneratedComment(
                content=result.get('comment', ''),
                style=style,
                image_prompt=result.get('image_prompt', ''),
                confidence=result.get('confidence', 0.5)
            )

        except Exception as e:
            print(f"è¯„è®ºç”Ÿæˆå¤±è´¥ ({style}): {e}")
            return None


class ImageGenerator:
    """å›¾ç‰‡ç”Ÿæˆå™¨"""

    def __init__(self, config: Dict):
        self.config = config
        # é»˜è®¤ä½¿ç”¨ GPT-5
        self.provider = config.get('image_provider', 'dalle')

        # éœ€è¦ OpenAI key çš„åˆ†æ”¯
        if self.provider in ['gpt5', 'dalle']:
            openai.api_key = config['openai_api_key']
        elif self.provider == 'flux':
            # Flux APIé…ç½®
            self.flux_api_key = config.get('flux_api_key')
        elif self.provider == 'firefly':
            # Adobe Firefly APIé…ç½®
            self.firefly_api_key = config.get('firefly_api_key')

    def generate_comment_image(self, comment: GeneratedComment, news_title: str) -> Optional[str]:
        """ä¸ºè¯„è®ºç”Ÿæˆé…å¥—å›¾ç‰‡"""

        # æ„å»ºå›¾ç‰‡ç”Ÿæˆæç¤º
        image_prompt = f"""
        è¯·æ ¹æ®ä»¥ä¸‹è¯„è®ºç”Ÿæˆä¸€å¼ å…·æœ‰æ¢—å›¾é£æ ¼çš„å›¾ç‰‡ï¼š

        è¯„è®ºå†…å®¹ï¼šã€Œ{comment.content}ã€
        è¯„è®ºé£æ ¼ï¼š{comment.style}
        æ–°é—»èƒŒæ™¯ï¼šã€Œ{news_title}ã€
        ç”»é¢æ„æ€ï¼š{comment.image_prompt}
        å¦‚æœéœ€è¦ç”Ÿæˆåäººè‚–åƒï¼Œè¯·ç”Ÿæˆå¡é€šç”»é£çš„åäººè‚–åƒï¼Œå‡†ç¡®æŒ‡å‡ºåäººçš„å§“åã€‚
        è¯·ç¡®ä¿å›¾ç‰‡å…·æœ‰è§†è§‰å†²å‡»åŠ›ï¼Œé€‚åˆåœ¨ç¤¾äº¤åª’ä½“ä¸Šä¼ æ’­ï¼Œå…¼å…·è¶£å‘³æ€§ä¸è¯é¢˜æ€§ã€‚é¿å…ä½¿ç”¨å¤ªå¤šçš„æ–‡å­—ï¼Œç®€å•æ˜äº†å³å¯ã€‚
        """

        try:
            if self.provider == 'gpt5':
                return self._generate_with_dalle(image_prompt)
            elif self.provider == 'dalle':
                return self._generate_with_dalle(image_prompt)
            elif self.provider == 'flux':
                return self._generate_with_flux(image_prompt)
            elif self.provider == 'firefly':
                return self._generate_with_firefly(image_prompt)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å›¾ç‰‡ç”Ÿæˆå™¨: {self.provider}")

        except Exception as e:
            print(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_text_image(comment)

    def _generate_with_gpt5(self, prompt: str) -> str:
        """ä½¿ç”¨ GPT-5 ç”Ÿæˆå›¾ç‰‡ï¼ˆè‹¥è´¦å·å·²å¼€é€š GPT-5 å›¾åƒèƒ½åŠ›ï¼‰"""
        # è‹¥ä½ çš„ GPT-5 è´¦æˆ·ä¸æ”¯æŒ images.generateï¼Œå¯åœ¨è¿™é‡Œæ”¹ä¸ºè°ƒç”¨ chat.completions + è‡ªå®šä¹‰æœåŠ¡å¯¹æ¥
        response = openai.images.generate(
            model="gpt-5",          # ç»Ÿä¸€æˆ gpt-5
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1
        )
        return response.data[0].url

    def _generate_with_dalle(self, prompt: str) -> str:
        """ä½¿ç”¨ DALLÂ·E 3 ç”Ÿæˆå›¾ç‰‡"""
        response = openai.images.generate(
            model="dall-e-3",
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1
        )
        return response.data[0].url

    def _generate_with_flux(self, prompt: str) -> str:
        """ä½¿ç”¨ Flux ç”Ÿæˆå›¾ç‰‡ï¼ˆç¤ºä¾‹ï¼Œæ ¹æ®å®é™… API è°ƒæ•´ï¼‰"""
        headers = {
            'Authorization': f'Bearer {self.flux_api_key}',
            'Content-Type': 'application/json'
        }
        data = {
            'prompt': prompt,
            'width': 1024,
            'height': 1024,
            'num_inference_steps': 28
        }
        response = requests.post('https://api.flux.ai/generate', headers=headers, json=data)
        return response.json()['url']

    def _generate_with_firefly(self, prompt: str) -> str:
        """ä½¿ç”¨ Adobe Firefly ç”Ÿæˆå›¾ç‰‡ï¼ˆç¤ºä¾‹ï¼Œæ ¹æ®å®é™… API è°ƒæ•´ï¼‰"""
        headers = {
            'Authorization': f'Bearer {self.firefly_api_key}',
            'Content-Type': 'application/json'
        }
        data = {
            'prompt': prompt,
            'size': {'width': 1024, 'height': 1024}
        }
        response = requests.post(
            'https://firefly-api.adobe.io/v1/images/generate',
            headers=headers, json=data
        )
        return response.json()['outputs'][0]['image']['url']

    def _generate_text_image(self, comment: GeneratedComment) -> str:
        """ç”Ÿæˆæ–‡å­—å¡ç‰‡å›¾ç‰‡ä½œä¸ºå¤‡é€‰"""
        try:
            img = Image.new('RGB', (800, 600), color='white')
            draw = ImageDraw.Draw(img)
            try:
                font = ImageFont.truetype("arial.ttf", 36)
                small_font = ImageFont.truetype("arial.ttf", 24)
            except:
                font = ImageFont.load_default()
                small_font = font

            lines = self._wrap_text(comment.content, 40)
            y = 200
            for line in lines:
                draw.text((50, y), line, font=font, fill='black')
                y += 50

            draw.text((50, 550), f"é£æ ¼: {comment.style}", font=small_font, fill='gray')

            buffer = io.BytesIO()
            img.save(buffer, format='PNG')
            img_str = base64.b64encode(buffer.getvalue()).decode()
            return f"data:image/png;base64,{img_str}"
        except Exception as e:
            print(f"æ–‡å­—å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def _wrap_text(self, text: str, width: int) -> List[str]:
        """æ–‡å­—æ¢è¡Œå¤„ç†"""
        lines = []
        words = text.split()
        current_line = []
        current_length = 0

        for word in words:
            if current_length + len(word) + 1 <= width:
                current_line.append(word)
                current_length += len(word) + 1
            else:
                if current_line:
                    lines.append(' '.join(current_line))
                current_line = [word]
                current_length = len(word)

        if current_line:
            lines.append(' '.join(current_line))
        return lines


class NewsCommentBot:
    """æ–°é—»è¯„è®ºæœºå™¨äººä¸»ç±»"""

    def __init__(self, config: Dict):
        self.extractor = NewsContentExtractor()
        self.analyzer = NewsAnalyzer(config['openai_api_key'])
        self.reddit_miner = RedditMiner(
            config['reddit_client_id'],
            config['reddit_client_secret'],
            config['reddit_user_agent']
        )
        self.comment_generator = CommentGenerator(config['openai_api_key'])
        self.image_generator = ImageGenerator(config)

    def process_news_url(self, url: str) -> Dict:
        """å¤„ç†æ–°é—»URLï¼Œç”Ÿæˆè¯„è®ºå’Œå›¾ç‰‡"""
        print(f"å¼€å§‹å¤„ç†æ–°é—»: {url}")

        try:
            # 1. æå–æ–°é—»å†…å®¹
            print("æå–æ–°é—»å†…å®¹...")
            news_data = self.extractor.extract_content(url)

            # 2. åˆ†ææ–°é—»å†…å®¹
            print("åˆ†ææ–°é—»å†…å®¹...")
            analysis = self.analyzer.analyze_news(news_data['title'], news_data['content'])

            # 3. æœç´¢ç›¸å…³Redditè¯„è®º
            print("æœç´¢Redditå‚è€ƒè¯„è®º...")
            keywords = analysis.core_viewpoints + [news_data['title'].split()[0]]
            reddit_refs = self.reddit_miner.find_related_discussions(keywords)

            # 4. ç”Ÿæˆè¯„è®º
            print("ç”ŸæˆåŸåˆ›è¯„è®º...")
            generated_comments = self.comment_generator.generate_comments(analysis, reddit_refs)

            # 5. ä¸ºæœ€ä½³è¯„è®ºç”Ÿæˆå›¾ç‰‡
            print("ç”Ÿæˆé…å¥—å›¾ç‰‡...")
            if generated_comments:
                for cmt in generated_comments:
                    image_url = self.image_generator.generate_comment_image(cmt, news_data['title'])
                    cmt.image_url = image_url
                    print("å›¾ç‰‡Url: " + image_url)

            # 6. æ•´ç†ç»“æœ
            result = {
                'news': news_data,
                'analysis': analysis,
                'reddit_references': reddit_refs,
                'generated_comments': generated_comments,
                'processing_time': datetime.now().isoformat(),
                'success': True
            }

            print("å¤„ç†å®Œæˆï¼")
            return result

        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")
            return {
                'error': str(e),
                'success': False
            }

    def export_result(self, result: Dict, filename: str = None):
        """å¯¼å‡ºç»“æœåˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"news_comments_{timestamp}.json"

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        export_data = {}

        if result.get('success'):
            export_data = {
                'news': result['news'],
                'analysis': {
                    'humor_points': result['analysis'].humor_points,
                    'criticism_points': result['analysis'].criticism_points,
                    'core_viewpoints': result['analysis'].core_viewpoints,
                    'controversial_points': result['analysis'].controversial_points,
                    'summary': result['analysis'].summary
                },
                'reddit_references': [
                    {
                        'comment': ref.comment,
                        'score': ref.score,
                        'style': ref.style,
                        'subreddit': ref.subreddit
                    }
                    for ref in result['reddit_references']
                ],
                'generated_comments': [
                    {
                        'content': comment.content,
                        'style': comment.style,
                        'image_prompt': comment.image_prompt,
                        'confidence': comment.confidence,
                        'image_url': getattr(comment, 'image_url', None)
                    }
                    for comment in result['generated_comments']
                ],
                'processing_time': result['processing_time']
            }
        else:
            export_data = {'error': result['error']}

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

        print(f"ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")


def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨é…ç½®æ–‡ä»¶ç³»ç»Ÿ"""

    # å¯¼å…¥é…ç½®ç®¡ç†å™¨
    try:
        from config_manager import ConfigManager
    except ImportError:
        print("âŒ è¯·ç¡®ä¿ config_manager.py æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹")
        print("æˆ–è€…æ‰‹åŠ¨é…ç½® config å˜é‡ï¼ˆå‚è€ƒåŸä»£ç ï¼‰")
        return

    try:
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        print("ğŸ”§ æ­£åœ¨åŠ è½½é…ç½®...")
        manager = ConfigManager()

        # éªŒè¯å¹¶è·å–é…ç½®
        config = manager.validate_and_setup()

        if not config['openai_api_key']:
            print("âŒ OpenAI APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•ç»§ç»­")
            return

        # åˆ›å»ºæœºå™¨äººå®ä¾‹
        print("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æ–°é—»è¯„è®ºç”Ÿæˆå™¨...")
        bot = NewsCommentBot(config)

        # äº¤äº’å¼å¤„ç†
        print("\n=== æ–°é—»è¯„è®ºç”Ÿæˆå™¨ ===")

        # åŠ è½½ç”¨æˆ·å†å²è¾“å…¥
        user_inputs = manager.loader.load_user_inputs()
        last_url = user_inputs.get('last_news_url', '')

        if last_url:
            print(f"ä¸Šæ¬¡å¤„ç†çš„é“¾æ¥: {last_url}")
            use_last = input("æ˜¯å¦ç»§ç»­ä½¿ç”¨ä¸Šæ¬¡çš„é“¾æ¥? (Y/n): ").strip().lower()
            if use_last in ['', 'y', 'yes']:
                news_url = last_url
            else:
                news_url = input("è¯·è¾“å…¥æ–°é—»é“¾æ¥: ").strip()
        else:
            news_url = input("è¯·è¾“å…¥æ–°é—»é“¾æ¥: ").strip()

        if not news_url:
            print("âŒ æœªè¾“å…¥æœ‰æ•ˆçš„æ–°é—»é“¾æ¥")
            return

        # ä¿å­˜å½“å‰URL
        user_inputs['last_news_url'] = news_url
        manager.loader.save_user_inputs(user_inputs)

        # å¤„ç†æ–°é—»
        print(f"\nğŸ”„ æ­£åœ¨å¤„ç†æ–°é—»: {news_url}")
        result = bot.process_news_url(news_url)

        if result.get('success'):
            print("\nâœ… å¤„ç†å®Œæˆï¼")

            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print("\n=== æ–°é—»åˆ†æç»“æœ ===")
            print(f"ğŸ“° æ ‡é¢˜: {result['news']['title']}")
            print(f"ğŸ“ æ‘˜è¦: {result['analysis'].summary}")

            # æ˜¾ç¤ºåˆ†æè¦ç‚¹
            analysis = result['analysis']
            if analysis.humor_points:
                print(f"ğŸ˜„ ç¬‘ç‚¹: {'; '.join(analysis.humor_points)}")
            if analysis.criticism_points:
                print(f"ğŸ¯ æ§½ç‚¹: {'; '.join(analysis.criticism_points)}")
            if analysis.controversial_points:
                print(f"âš¡ äº‰è®®ç‚¹: {'; '.join(analysis.controversial_points)}")

            # æ˜¾ç¤ºç”Ÿæˆçš„è¯„è®º
            print(f"\n=== ç”Ÿæˆçš„è¯„è®º ({len(result['generated_comments'])} ä¸ª) ===")
            for i, comment in enumerate(result['generated_comments'], 1):
                style_emoji = {
                    'provocative': 'ğŸ”¥',
                    'witty': 'ğŸ˜„',
                    'insightful': 'ğŸ§ ',
                    'question': 'â“'
                }.get(comment.style, 'ğŸ’¬')

                print(f"\n{i}. {style_emoji} ã€{comment.style}ã€‘(è¯„åˆ†: {comment.confidence:.2f})")
                print(f"ğŸ’¬ {comment.content}")
                print(f"ğŸ¨ å›¾ç‰‡åˆ›æ„: {comment.image_prompt}")

                if hasattr(comment, 'image_url') and comment.image_url:
                    print(f"ğŸ–¼ï¸  é…å¥—å›¾ç‰‡: {comment.image_url}")

            # Redditå‚è€ƒ
            if result['reddit_references']:
                print(f"\n=== Redditçƒ­é—¨å‚è€ƒ ({len(result['reddit_references'])} æ¡) ===")
                for i, ref in enumerate(result['reddit_references'][:3], 1):  # åªæ˜¾ç¤ºå‰3æ¡
                    print(f"{i}. [r/{ref.subreddit}] (ğŸ‘{ref.score}) {ref.comment[:100]}...")

            # å¯¼å‡ºç»“æœ
            print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡ºç»“æœ...")
            bot.export_result(result)

            # è¯¢é—®æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–æ–°é—»
            continue_processing = input("\næ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–æ–°é—»? (y/N): ").strip().lower()
            if continue_processing in ['y', 'yes']:
                main()  # é€’å½’è°ƒç”¨

        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {result['error']}")

            # æä¾›æ•…éšœæ’é™¤å»ºè®®
            print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
            print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("2. ç¡®è®¤APIå¯†é’¥æ­£ç¡®")
            print("3. æ£€æŸ¥æ–°é—»é“¾æ¥æ˜¯å¦å¯è®¿é—®")
            print("4. æŸ¥çœ‹é…ç½®æ–‡ä»¶è®¾ç½®")

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œç½‘ç»œè¿æ¥")


if __name__ == "__main__":
    main()
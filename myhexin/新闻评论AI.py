#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–°é—»è¯„è®ºç”Ÿæˆå™¨
åŠŸèƒ½ï¼šåˆ†ææ–°é—»å†…å®¹ï¼Œç”Ÿæˆæœ‰è¶£çš„è¯„è®ºï¼Œå¹¶é…å¥—ç›¸å…³å›¾ç‰‡
"""

import requests
import json
import re
import time
import random
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import openai
from PIL import Image, ImageDraw, ImageFont
import io
import base64
from typing import Dict, List, Optional, Tuple
import praw
from dataclasses import dataclass
from datetime import datetime


@dataclass
class NewsAnalysis:
    """æ–°é—»åˆ†æç»“æœæ•°æ®ç±»"""
    humor_points: List[str]  # ç¬‘ç‚¹
    criticism_points: List[str]  # æ§½ç‚¹
    core_viewpoints: List[str]  # æ ¸å¿ƒè§‚ç‚¹
    controversial_points: List[str]  # äº‰è®®ç‚¹
    key_images: List[str]  # å…³é”®å›¾ç‰‡URL
    summary: str  # æ–‡ç« æ‘˜è¦


@dataclass
class RedditReference:
    """Redditå‚è€ƒè¯„è®ºæ•°æ®ç±»"""
    comment: str
    score: int
    awards: int
    subreddit: str
    style: str  # è¯„è®ºé£æ ¼æ ‡ç­¾


@dataclass
class GeneratedComment:
    """ç”Ÿæˆçš„è¯„è®ºæ•°æ®ç±»"""
    content: str
    style: str  # é£æ ¼ï¼šprovocative, witty, insightful, question
    image_prompt: str  # é…å¥—å›¾ç‰‡æè¿°
    confidence: float  # è´¨é‡è¯„åˆ†


class NewsContentExtractor:
    """æ–°é—»å†…å®¹æå–å™¨"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def extract_content(self, url: str) -> Dict:
        """æå–æ–°é—»å†…å®¹å’Œå›¾ç‰‡"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            # æå–æ ‡é¢˜
            title = self._extract_title(soup)

            # æå–æ­£æ–‡
            content = self._extract_article_text(soup)

            # æå–å›¾ç‰‡
            images = self._extract_images(soup, url)

            return {
                'title': title,
                'content': content,
                'images': images,
                'url': url
            }

        except Exception as e:
            raise Exception(f"å†…å®¹æå–å¤±è´¥: {str(e)}")

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–æ ‡é¢˜"""
        selectors = ['h1', '.title', '#title', '[class*="title"]', 'title']
        for selector in selectors:
            element = soup.select_one(selector)
            if element and element.get_text().strip():
                return element.get_text().strip()
        return "æœªæ‰¾åˆ°æ ‡é¢˜"

    def _extract_article_text(self, soup: BeautifulSoup) -> str:
        """æå–æ–‡ç« æ­£æ–‡"""
        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()

        # å°è¯•å¤šç§æ­£æ–‡é€‰æ‹©å™¨
        content_selectors = [
            '[class*="content"]', '[class*="article"]', '[class*="story"]',
            '[id*="content"]', '[id*="article"]', 'main', '.post-content'
        ]

        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                text = ' '.join([elem.get_text() for elem in elements])
                if len(text) > 200:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                    return self._clean_text(text)

        # å¤‡ç”¨æ–¹æ¡ˆï¼šæå–æ‰€æœ‰pæ ‡ç­¾
        paragraphs = soup.find_all('p')
        if paragraphs:
            text = ' '.join([p.get_text() for p in paragraphs])
            return self._clean_text(text)

        return soup.get_text()[:2000]  # é™åˆ¶é•¿åº¦

    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """æå–å›¾ç‰‡URL"""
        images = []
        img_tags = soup.find_all('img')

        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src:
                full_url = urljoin(base_url, src)
                if self._is_valid_image(full_url):
                    images.append(full_url)

        return images[:5]  # é™åˆ¶å›¾ç‰‡æ•°é‡

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶ç©ºç™½å­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()"""''â€”-]', '', text)  # ä¿ç•™ä¸­è‹±æ–‡å’Œå¸¸ç”¨æ ‡ç‚¹
        return text.strip()

    def _is_valid_image(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå›¾ç‰‡URL"""
        valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
        return any(url.lower().endswith(ext) for ext in valid_extensions)


class NewsAnalyzer:
    """æ–°é—»å†…å®¹åˆ†æå™¨"""

    def __init__(self, api_key: str):
        openai.api_key = api_key

    def analyze_news(self, title: str, content: str) -> NewsAnalysis:
        """åˆ†ææ–°é—»å†…å®¹ï¼Œè¯†åˆ«å„ç§è¦ç‚¹"""

        prompt = f"""
        è¯·ç”¨ä¸­æ–‡åˆ†æä»¥ä¸‹æ–°é—»å†…å®¹ï¼Œè¯†åˆ«å‡ºï¼š
        1. ç¬‘ç‚¹ï¼ˆæœ‰è¶£ã€æç¬‘çš„éƒ¨åˆ†ï¼‰
        2. æ§½ç‚¹ï¼ˆå€¼å¾—åæ§½ã€æ‰¹è¯„çš„åœ°æ–¹ï¼‰
        3. æ ¸å¿ƒè§‚ç‚¹ï¼ˆæ–‡ç« çš„ä¸»è¦è®ºç‚¹ï¼‰
        4. æ½œåœ¨äº‰è®®ç‚¹ï¼ˆå¯èƒ½å¼•èµ·äº‰è®®çš„å†…å®¹ï¼‰

        æ ‡é¢˜ï¼š{title}
        å†…å®¹ï¼š{content[:2000]}

        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        - humor_points: ç¬‘ç‚¹åˆ—è¡¨
        - criticism_points: æ§½ç‚¹åˆ—è¡¨
        - core_viewpoints: æ ¸å¿ƒè§‚ç‚¹åˆ—è¡¨
        - controversial_points: äº‰è®®ç‚¹åˆ—è¡¨
        - summary: æ–‡ç« æ‘˜è¦ï¼ˆ50å­—ä»¥å†…ï¼‰
        """

        try:
            response = openai.chat.completions.create(
                model="gpt-5",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆï¼Œå–„äºè¯†åˆ«æ–°é—»ä¸­çš„å„ç§è¦ç‚¹ã€‚è¯·ç”¨ä¸­æ–‡åˆ†æï¼Œå¹¶è¯·å§‹ç»ˆè¿”å›æ ‡å‡†JSONæ ¼å¼å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ³¨é‡Šæˆ–markdownæ ¼å¼ï¼Œä¸åŠ è§£é‡Šå’Œä»»ä½•è¯´æ˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7
            )
            raw_content = response.choices[0].message.content.strip()

            # å¦‚æœä»¥```å¼€å¤´ï¼Œå»é™¤Markdownä»£ç å—
            if raw_content.startswith("```"):
                raw_content = re.sub(r"^```(?:json)?\s*", "", raw_content)  # ç§»é™¤èµ·å§‹ ```
                raw_content = re.sub(r"\s*```$", "", raw_content)  # ç§»é™¤ç»“æŸ ```

            result = json.loads(raw_content)

            return NewsAnalysis(
                humor_points=result.get('humor_points', []),
                criticism_points=result.get('criticism_points', []),
                core_viewpoints=result.get('core_viewpoints', []),
                controversial_points=result.get('controversial_points', []),
                key_images=[],  # å°†åœ¨åç»­å¡«å……
                summary=result.get('summary', '')
            )

        except Exception as e:
            print(f"æ–°é—»åˆ†æå¤±è´¥: {e}")
            return NewsAnalysis([], [], [], [], [], "åˆ†æå¤±è´¥")


class RedditMiner:
    """Redditè¯„è®ºæŒ–æ˜å™¨"""

    def __init__(self, client_id: str, client_secret: str, user_agent: str):
        self.reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )

    def find_related_discussions(self, keywords: List[str], limit: int = 10) -> List[RedditReference]:
        """æ ¹æ®å…³é”®è¯æœç´¢ç›¸å…³Redditè®¨è®º"""
        references = []

        # ç»„åˆå…³é”®è¯è¿›è¡Œæœç´¢
        query = ' OR '.join(keywords[:3])  # é™åˆ¶å…³é”®è¯æ•°é‡

        try:
            # æœç´¢çƒ­é—¨å¸–å­
            for submission in self.reddit.subreddit('all').search(query, limit=limit, sort='hot'):
                # è·å–é«˜è´¨é‡è¯„è®º
                submission.comments.replace_more(limit=0)  # å±•å¼€è¯„è®º

                for comment in submission.comments[:5]:  # å–å‰5ä¸ªè¯„è®º
                    if len(comment.body) > 50 and comment.score > 10:
                        style = self._classify_comment_style(comment.body)

                        references.append(RedditReference(
                            comment=comment.body[:300],  # é™åˆ¶é•¿åº¦
                            score=comment.score,
                            awards=len(comment.all_awardings) if hasattr(comment, 'all_awardings') else 0,
                            subreddit=submission.subreddit.display_name,
                            style=style
                        ))

            # æŒ‰çƒ­åº¦æ’åº
            references.sort(key=lambda x: x.score + x.awards * 10, reverse=True)
            return references[:5]  # è¿”å›æœ€çƒ­é—¨çš„5ä¸ª

        except Exception as e:
            print(f"Redditæœç´¢å¤±è´¥: {e}")
            return []

    def _classify_comment_style(self, comment: str) -> str:
        """åˆ†ç±»è¯„è®ºé£æ ¼"""
        comment_lower = comment.lower()

        # å¼•æˆ˜å‹
        if any(word in comment_lower for word in ['stupid', 'wrong', 'disagree', 'ridiculous']):
            return 'provocative'

        # å¹½é»˜å‹
        elif any(word in comment_lower for word in ['lol', 'funny', 'joke', 'haha', 'ğŸ˜‚']):
            return 'witty'

        # æé—®å‹
        elif comment.count('?') >= 2:
            return 'question'

        # æ·±åˆ»å‹
        elif len(comment) > 200:
            return 'insightful'

        return 'neutral'


class CommentGenerator:
    """è¯„è®ºç”Ÿæˆå™¨"""

    def __init__(self, api_key: str):
        openai.api_key = api_key
        self.styles = ['provocative', 'witty', 'insightful', 'question']

    def generate_comments(self, analysis: NewsAnalysis, reddit_refs: List[RedditReference]) -> List[GeneratedComment]:
        """ç”Ÿæˆå¤šç§é£æ ¼çš„è¯„è®º"""
        comments = []

        # ä¸ºæ¯ç§é£æ ¼ç”Ÿæˆä¸€ä¸ªè¯„è®º
        for style in self.styles:
            comment = self._generate_single_comment(analysis, reddit_refs, style)
            if comment:
                comments.append(comment)

        return sorted(comments, key=lambda x: x.confidence, reverse=True)

    def _generate_single_comment(self, analysis: NewsAnalysis, reddit_refs: List[RedditReference], style: str) -> \
    Optional[GeneratedComment]:
        """ç”Ÿæˆç‰¹å®šé£æ ¼çš„è¯„è®º"""

        # è·å–ç›¸åŒé£æ ¼çš„Redditå‚è€ƒ
        style_refs = [ref for ref in reddit_refs if ref.style == style]
        ref_text = '\n'.join([ref.comment for ref in style_refs[:2]])

        # æ„å»ºæç¤º
        style_prompts = {
            'provocative': 'ç”Ÿæˆä¸€ä¸ªæœ‰äº‰è®®æ€§ã€èƒ½å¤Ÿå¼•èµ·è®¨è®ºçš„è¯„è®ºï¼Œè¦çŠ€åˆ©ä½†ä¸è¿‡åˆ†',
            'witty': 'ç”Ÿæˆä¸€ä¸ªæœºæ™ºå¹½é»˜çš„è¯„è®ºï¼Œå¯ä»¥æ˜¯åŒå…³è¯­ã€è®½åˆºæˆ–å·§å¦™çš„è§‚å¯Ÿ',
            'insightful': 'ç”Ÿæˆä¸€ä¸ªæ·±åˆ»æœ‰è§åœ°çš„è¯„è®ºï¼Œæä¾›æ–°çš„è§†è§’æˆ–æ·±åº¦åˆ†æ',
            'question': 'ç”Ÿæˆä¸€ä¸ªå‘äººæ·±çœçš„é—®é¢˜ï¼Œèƒ½å¤Ÿå¼•å‘è¯»è€…æ€è€ƒ'
        }

        prompt = f"""
        # è§’è‰²
        ä½ æ˜¯ä¸€åæ´å¯ŸåŠ›æ•é”ã€è¯­è¨€é£è¶£çš„ç¤¾äº¤åª’ä½“è¯„è®ºå‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºä¸€ä»½æ–°é—»çš„æ ¸å¿ƒåˆ†æï¼Œåˆ›ä½œå‡ºä¸€æ¡ç®€çŸ­ã€çŠ€åˆ©ã€æ˜“äºä¼ æ’­çš„åŸåˆ›è¯„è®ºï¼Œå¹¶ä¸ºä¹‹æ„æ€ä¸€ä¸ªå¯Œæœ‰åˆ›æ„çš„å›¾ç‰‡æè¿°ã€‚

        # æ–°é—»åˆ†æææ–™
        - **æ ¸å¿ƒè§‚ç‚¹**: {'; '.join(analysis.core_viewpoints)}
        - **äº‰è®®ä¸æ§½ç‚¹**: {'; '.join(analysis.controversial_points + analysis.criticism_points)}
        - **å¹½é»˜ä¸ç¬‘ç‚¹**: {'; '.join(analysis.humor_points)}

        # åˆ›ä½œæŒ‡ä»¤
        1.  **è¯„è®ºå†…å®¹**:
            - **é£æ ¼**: æ•´ä½“åŸºè°ƒä¸º **{style_prompts[style]}**ã€‚
            - **å‚è€ƒ**: æ¨¡ä»¿ä»¥ä¸‹ä¾‹å­çš„è¯­æ°”å’Œæ€åº¦ï¼Œä½†**ä¸è¦ç…§æŠ„**å†…å®¹æˆ–å¥å¼: "{ref_text}"
            - **æ•´åˆ**: å·§å¦™åœ°èåˆ`æ–°é—»åˆ†æææ–™`ä¸­çš„è‡³å°‘ä¸€åˆ°ä¸¤ä¸ªå…³é”®ç‚¹ï¼Œå½¢æˆè¿è´¯ã€æœ‰è§åœ°çš„è§‚ç‚¹ã€‚é¿å…ç”Ÿç¡¬ç½—åˆ—ã€‚
            - **çº¦æŸ**: é•¿åº¦ä¸¥æ ¼æ§åˆ¶åœ¨100å­—ä»¥å†…ï¼Œè¯­è¨€è‡ªç„¶å£è¯­åŒ–ï¼Œç¬¦åˆç½‘ç»œè®¨è®ºä¹ æƒ¯ã€‚

        2.  **å›¾ç‰‡æè¿° (Image Prompt)**:
            - **è¦æ±‚**: æ„æ€ä¸€ä¸ªå…·æœ‰è±¡å¾æ€§ã€è®½åˆºæ€§æˆ–æˆå‰§æ€§å¼ åŠ›çš„ç”»é¢ï¼Œèƒ½å¤Ÿå°†ä½ çš„è¯„è®ºæ ¸å¿ƒæ€æƒ³è§†è§‰åŒ–ã€‚æè¿°éœ€è¦ç”ŸåŠ¨ã€å¯Œæœ‰æƒ³è±¡åŠ›ï¼Œä¾¿äºAIç»˜ç”»æ¨¡å‹ç†è§£ã€‚

        # è¾“å‡ºæ ¼å¼
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•JSONæ ¼å¼ä¹‹å¤–çš„é¢å¤–è§£é‡Šã€‚
        {{
            "comment": "åœ¨è¿™é‡Œå¡«å†™ä½ åˆ›ä½œçš„è¯„è®ºå†…å®¹",
            "image_prompt": "åœ¨è¿™é‡Œå¡«å†™ä½ æ„æ€çš„å›¾ç‰‡æè¿°",
            "confidence": 0.8
        }}
        """

        try:
            response = openai.chat.completions.create(
                model="gpt-5",
                messages=[
                    {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ª{style}é£æ ¼çš„ç½‘ç»œè¯„è®ºé«˜æ‰‹ã€‚è¯·å§‹ç»ˆè¿”å›æ ‡å‡†JSONæ ¼å¼å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•æ³¨é‡Šæˆ–markdownæ ¼å¼ï¼Œä¸åŠ è§£é‡Šå’Œä»»ä½•è¯´æ˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8
            )

            raw_content = response.choices[0].message.content.strip()

            # å¦‚æœä»¥```å¼€å¤´ï¼Œå»é™¤Markdownä»£ç å—
            if raw_content.startswith("```"):
                raw_content = re.sub(r"^```(?:json)?\s*", "", raw_content)  # ç§»é™¤èµ·å§‹ ```
                raw_content = re.sub(r"\s*```$", "", raw_content)  # ç§»é™¤ç»“æŸ ```

            result = json.loads(raw_content)

            return GeneratedComment(
                content=result.get('comment', ''),
                style=style,
                image_prompt=result.get('image_prompt', ''),
                confidence=result.get('confidence', 0.5)
            )

        except Exception as e:
            print(f"è¯„è®ºç”Ÿæˆå¤±è´¥ ({style}): {e}")
            return None


class ImageGenerator:
    """å›¾ç‰‡ç”Ÿæˆå™¨"""

    def __init__(self, config: Dict):
        self.config = config
        self.provider = config.get('image_provider', 'gpt5')  # é»˜è®¤ä½¿ç”¨GPT-5

        if self.provider in ['gpt5', 'dalle']:
            openai.api_key = config['openai_api_key']
        elif self.provider == 'flux':
            # Flux APIé…ç½®
            self.flux_api_key = config.get('flux_api_key')
        elif self.provider == 'firefly':
            # Adobe Firefly APIé…ç½®
            self.firefly_api_key = config.get('firefly_api_key')

    def generate_comment_image(self, comment: GeneratedComment, news_title: str) -> Optional[str]:
        """ä¸ºè¯„è®ºç”Ÿæˆé…å¥—å›¾ç‰‡"""

        # æ„å»ºå›¾ç‰‡ç”Ÿæˆæç¤º
        image_prompt = f"""
        è¯·æ ¹æ®ä»¥ä¸‹è¯„è®ºç”Ÿæˆä¸€å¼ å…·æœ‰æ¢—å›¾é£æ ¼çš„å›¾ç‰‡ï¼š

        è¯„è®ºå†…å®¹ï¼šã€Œ{comment.content}ã€
        è¯„è®ºé£æ ¼ï¼š{comment.style}
        æ–°é—»èƒŒæ™¯ï¼šã€Œ{news_title}ã€
        ç”»é¢æ„æ€ï¼š{comment.image_prompt}
        å¦‚æœéœ€è¦ç”Ÿæˆåäººè‚–åƒï¼Œè¯·ç”Ÿæˆå¡é€šç”»é£çš„åäººè‚–åƒï¼Œå‡†ç¡®æŒ‡å‡ºåäººçš„å§“åã€‚
        è¯·ç¡®ä¿å›¾ç‰‡å…·æœ‰è§†è§‰å†²å‡»åŠ›ï¼Œé€‚åˆåœ¨ç¤¾äº¤åª’ä½“ä¸Šä¼ æ’­ï¼Œå…¼å…·è¶£å‘³æ€§ä¸è¯é¢˜æ€§ã€‚é¿å…ä½¿ç”¨å¤ªå¤šçš„æ–‡å­—ï¼Œç®€å•æ˜äº†å³å¯ã€‚
        """

        try:
            if self.provider == 'gpt5':
                return self._generate_with_gpt5(image_prompt)
            elif self.provider == 'dalle':
                return self._generate_with_dalle(image_prompt)
            elif self.provider == 'flux':
                return self._generate_with_flux(image_prompt)
            elif self.provider == 'firefly':
                return self._generate_with_firefly(image_prompt)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å›¾ç‰‡ç”Ÿæˆå™¨: {self.provider}")

        except Exception as e:
            print(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_text_image(comment)